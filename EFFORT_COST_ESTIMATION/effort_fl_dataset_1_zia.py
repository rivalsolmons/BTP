# -*- coding: utf-8 -*-
"""Effort_Fl_dataset_1_Zia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TbEj0Fo9YwlZpCW4iIVgv8a40n-LUJPw

# Effort and cost Estimation in Agile Software Development
"""

#@title Cost Estimation on Zia 
import math
import pandas as pd
import numpy as np

from scipy.stats import pearsonr
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from matplotlib.colors import ListedColormap  
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
from sklearn.metrics import r2_score
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

data_read = pd.read_csv('/content/drive/MyDrive/Btp/Ultra/Zia_upd2.csv')

data_read.head()

#@title Improving The data Qualitity 
#@markdown

#@title Defining Category 1 and Category 2


features = ['Actual_Cost'	,'Vi',	'D',	'V',		'Work_days',	'Team_Salary',	'Act_Time']
#features = ['Actual_Cost',	'Vi',	'D',	'V',	'Sprint_Size',	'Work_days',	'Act_Time'] 



Cat_1 = data_read[features]
Cat_2 = data_read['Effort']

#@title Describing Categorical Data 
Cat_1.describe()



#For debugging purpose
#print(Cat_1)

#@title Describe the raw Data of splitted trained set from the dataframe
display(Cat_1)



print('The overall nummber of projects are: ',len(Cat_2))

display(Cat_2)

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(Cat_1,Cat_2,test_size=0.20, random_state=0)
from sklearn.linear_model import LinearRegression
ml = LinearRegression()
ml.fit(x_train,y_train)
ml.predict(x_test)
y_pred1 = ml.predict(x_test)

ypredx=len(x_test)
## For Debugging Purpose


# print(len(y_pred1))
# print('xtrain',len(x_train))
# print('xtest',len(x_test))

import math
print(y_test)

print(y_pred1)

ml.predict(x_train)
x_train_pred_lr= ml.predict(x_train)

import matplotlib.pyplot as plt

plt.figure(figsize=(7,7))
plt.scatter(y_train, x_train_pred_lr ,label='Train Data',color='red' )
plt.scatter(y_test, y_pred1)


#plt.scatter()
plt.xlabel('Actual Cost of the project')
plt.ylabel('Predicted Cost of the project')
plt.title('Cost Actual vs Predicted Linear Regresssion')

arr =pd.DataFrame({'Actual  Cost':y_test })
arr1 =pd.DataFrame({'Predicted Cost':y_pred1})

"""# Now calculating Diffrence between actual and predicted Values"""

import pandas as pd
pred_y_diff = pd.DataFrame({'Actual Values of Cost':y_test,'Predicted Cost':y_pred1%10**5,'Difference':y_test-y_pred1})



pred_y_diff[0:21]

#arr = pred_y_diff.to_numpy()
arr =pd.DataFrame({'Actual  Cost':y_test })
arr1 =pd.DataFrame({'Predicted Cost':y_pred1})
arr2 =pd.DataFrame({'Difference':y_test-y_pred1})

print(arr)

print(y_pred1)

# The r_sq
print('The r Score on the predictrd trained Set: %.2f'% r2_score(y_test,y_pred1))

diff=np.absolute(y_test-y_pred1)
MRE=np.absolute(diff)/y_test
MMRE=np.mean(MRE)


Accuracy=(1-MRE)*100
Accuracy=(Accuracy.mean())
MRE=(diff/y_test)/MRE.size

MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
P2=MRE[MRE<MMRE]

Pred=(P.size/MRE.size) * 100
Pred2=(P2.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)
print("PRED MMRE",Pred2)
print("Model Accuracy",Accuracy)


#linear

# @title KNeiboursRegressor 
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split

#print('x_train length :',len(x_train))
x_train,x_test,y_train,y_test = train_test_split(Cat_1,Cat_2, test_size=0.20, random_state=0)
#print(len(y_pred1))
#print('x_train length:',len(x_train))
#print('x_test length:',len(x_test))

neigh = KNeighborsRegressor(n_neighbors=2, weights='uniform')
neigh.fit(x_train,y_train)
y_pred=neigh.predict(x_test) 

#print(classification_report(y_test,y_pred))

#predict =neigh.predict(y_pred)
#print(neigh.score(y_test, y_pred))

import numpy as np
from sklearn.metrics import mean_absolute_error
#

print(len(y_pred))
print("Mean Absolute error:::",mean_absolute_error(y_test,y_pred))
#print("Mean Absolute error:::",np.sqrt(mean_absolute_error(y_test,y_pred)))


# The r_sq
print('The r Score on the predictrd trained Set: %.2f'% r2_score(y_test,y_pred))



#knn

#plotting the observed and predicted values 


# Setting the boundaries and parameters

plt.rcParams['figure.figsize'] = (10,6)
x_ax =range(len(x_test))
#x_ax2 =range(len(y_test))
#plotting 
plt.plot(x_ax, y_test , label='Actual Observed Value', color='yellow',linestyle='-')
plt.plot(x_ax,y_pred , label='Predicted values ' , color='red' , linestyle ='--')
plt.grid()
plt.xlabel('Agile Projects')
plt.ylabel('Cost of the project')
plt.title('Cost Prediction with KNeighbors Regressor')
plt.legend(bbox_to_anchor =(0.5,-0.2), loc= 'upper center',ncol=2, frameon = True)

plt.show

#
diff=np.absolute(y_test-y_pred)
MRE=diff/y_test
MRE=(diff/y_test)/MRE.size
MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
Pred=(P.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)

import pandas as pd

pred_y_diff = pd.DataFrame({'Actual Values of Cost':y_test,'Predicted Cost':y_pred,'Difference':y_test-y_pred})
xyy=pred_y_diff.Difference.abs()
pred_y_diff[0:21]

pred_mre = pd.DataFrame({'Actual Values of Cost':y_test,'Predicted Cost':y_pred,'MRE':(y_test-y_pred)/y_test})


pred_mre[0:21]

"""----------------------------

Decision Tree Implementation

## Decision Tree Implementation
"""

data_read.head()

#Now to Prepare the Input and Output Features
X= data_read.drop(['Effort'],axis=1)
X.shape

#Splitting the dataset into Trainingfset and test set

#Splitting the dataset
X_train,X_test,y_train,y_test = train_test_split(Cat_1,Cat_2, test_size=0.20 ,random_state=0)

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
import pandas as pd
#Creating and Fitting the model
regressor1 = DecisionTreeRegressor().fit(X_train,y_train)



# For the training model EVALUATION
from  sklearn.metrics import mean_absolute_error , mean_squared_error, explained_variance_score , r2_score

#prediction on the training Dataset
y_train_pred = regressor1.predict(X_train)
y_test_pred_dt = regressor1.predict(X_test)

# For the training model EVALUATION
from  sklearn.metrics import mean_absolute_error , mean_squared_error, explained_variance_score , r2_score

# The r_sq
print('The r Score on the predictrd trained Set: %.2f'% r2_score(y_test,y_test_pred_dt))

diff=np.absolute(y_test-y_test_pred_dt)
MRE=(diff/y_test)/MRE.size
MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
P2=MRE[MRE<MMRE]

Pred2=(P2.size/MRE.size) * 100
Pred=(P.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)
print("PRED MMRE",Pred2)
print("Model Accuracy",Accuracy)

# plotting the observed and predicted values 


# Setting the boundaries and parameters

plt.rcParams['figure.figsize'] = (10,6)
x_ax =range(len(X_test))
#x_ax2 =range(len(y_test))
#plotting 
plt.plot(x_ax, y_test , label='Actual Observed Value', color='yellow',linestyle='-')
plt.plot(x_ax,y_test_pred_dt , label='Predicted values ' , color='red' , linestyle ='--')
plt.grid()
plt.xlabel('Agile Projects')
plt.ylabel('Cost Parameter')
plt.title('Effort Prediction witn Decision Tree Regression')
plt.legend(bbox_to_anchor =(0.5,-0.2), loc= 'upper center',ncol=2, frameon = True)

plt.show

from yellowbrick.model_selection import ValidationCurve



# Creating the validation curve
import numpy as np
visualizer = ValidationCurve(DecisionTreeRegressor(), 
                             param_name="max_depth", n_jobs=-1,
                             param_range=np.arange(1, 22), 
                             cv=10, scoring="accuracy")

visualizer.fit(y_train, y_train_pred)

# Saving plot in PNG format
visualizer.show(outpath="Validation_Curve.png")

pred_mre = pd.DataFrame({'Actual Values of Effort':y_test,'Predicted Effort':y_test_pred_dt,'MRE':(y_test-y_test_pred_dt)/y_test})
#rel=(xyy/5)
#print(sum(rel)/5)
pred_mre[0:21]

import pandas as pd
pred_y_diff = pd.DataFrame({'Actual Values of Effort':y_test,'Predicted Effort':y_test_pred_dt,'Difference':y_test-y_test_pred_dt})

pred_y_diff[0:21]

#@title Random Forest Regresssor Implementation
from sklearn.model_selection  import train_test_split
X_train,X_test,y_train,y_test = train_test_split(Cat_1,Cat_2, test_size=0.20 ,random_state=0)

from sklearn.ensemble import RandomForestRegressor
model2= RandomForestRegressor(n_estimators=30, random_state=30)

#Fitting the  model 
LL_rf = model2.fit(X_train,y_train)

#prediction on testing the data
y_test_pred_rf = model2.predict(X_test)

#Training Model Evaluation


print('The Model Score on the predictrd training Set: %.2f'% LL_rf.score(X_train,y_train))
print('r2 Score ', r2_score(y_test,y_test_pred_rf))

# print("Mean Squared Error::",mean_squared_error(y_test,y_test_pred_rf))
# print("Mean Absolute Error::",mean_absolute_error(y_test,y_test_pred_rf))
# xv= math.sqrt(mean_squared_error(y_test,y_test_pred_rf))
# print("Root Mean Squared Error::",xv)

# plotting the observed and predicted values 


# Setting the boundaries and parameters

plt.rcParams['figure.figsize'] = (10,6)
x_ax =range(len(X_test))

#plotting 
plt.plot(x_ax, y_test , label='Actual Observed Value', color='yellow',linestyle='-')
plt.plot(x_ax,y_test_pred_rf , label='Predicted values ' , color='red' , linestyle ='--')
plt.grid()
plt.xlabel('Agile Projects')
plt.ylabel('Cost Parameter')
plt.title('Cost Prediction with Random Forest')
plt.legend(bbox_to_anchor =(0.5,-0.2), loc= 'upper center',ncol=2, frameon = True)

pred_mre = pd.DataFrame({'Actual Values of Cost':y_test,'Predicted Cost':y_test_pred_rf,'MRE':(y_test-y_test_pred_rf)/y_test})
#rel=(xyy/5)
#print(sum(rel)/5)
pred_mre[0:21]

"""Evaluation Parameters"""

# The r_sq
print('The r Score on the predictrd trained Set: %.2f'% r2_score(y_test,y_test_pred_rf))
diff=np.absolute(y_test-y_test_pred_rf)
MRE=diff/y_test


MRE=(diff/y_test)/MRE.size
MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
Pred=(P.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)
Accuracy=(1-MRE)*100
Accuracy=(Accuracy.mean())
MRE=(diff/y_test)/MRE.size
P2=MRE[MRE<MMRE]
print("PRED MMRE",Pred2)
print("Model Accuracy",Accuracy)
#

#@title ADA BOOST
from sklearn.ensemble import AdaBoostRegressor


# Categorical To continious 
X_train_ada,X_test_ada,y_train_ada,y_test_ada = train_test_split(Cat_1,Cat_2, test_size=0.20, random_state=0)


ada = AdaBoostRegressor()
ada.fit(X_train_ada,y_train_ada)

#clf = svm.SVC()
#clf.fit(Cat_1, Cat_2)
y_train_pred_ada=ada.predict(X_train_ada)
y_pred_ada=ada.predict(X_test_ada)



# The r_sq
print('The r Score on the predictrd test Set: %.2f'% r2_score(y_test_ada,y_pred_ada))
#print('The r Score on the predictrd trained Set: %.2f'% r2_score(y_train_ada,y_train_pred_ada))

diff=np.absolute(y_test_ada-y_pred_ada)
# MRE=diff/y_test
MRE=(diff/y_test)/MRE.size
MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
Pred=(P.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)
Accuracy=(1-MRE)*100
Accuracy=(Accuracy.mean())
MRE=(diff/y_test)/MRE.size
P2=MRE[MRE<MMRE]
print("PRED MMRE",Pred2)
print("Model Accuracy",Accuracy)
#

import matplotlib.pyplot as plt
import seaborn as sns

colors = sns.color_palette("colorblind")
# X_train_ada,X_test_ada,y_train_ada,y_test_ada
plt.figure()
plt.scatter(y_train_pred_ada,y_train_ada , label="training samples")
#plt.plot(X_test_ada,y_test_ada, color=colors[1], label="n_estimators=1", linewidth=2)
# plt.plot(X, y_2, color=colors[2], label="n_estimators=300", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("ADA BOOST REGRESSOR - Cost")
plt.legend()
plt.show()

#prediction on testing the data
#y_test_pred_svm = y_pred111.predict(y_test,y_test_pred_svm)

#@title Ensemble Learning 

data_read.head()

from sklearn.preprocessing import LabelEncoder

# encoder = LabelEncoder()

# data_read['Actual_Cost'] = encoder.fit_transform(data_read['Actual_Cost'])

dff = data_read.sample(21)
dff_train = dff.iloc[:14,:].sample(10)
dff_val = dff.iloc[14:18,:].sample(4)
dff_test = dff.iloc[17:,:].sample(3)

dff_train

X_test = dff_val.iloc[:,0:8].values
y_test = dff_val.iloc[:-1].values

# len(X_test)
# len(y_test)

#@title Bagging with 3 Decision Trees 
df_bag = dff_train.sample(7,replace=True) 

X= df_bag.iloc[:,0:8]
Y= df_bag.iloc[:,-1]

df_bag

X

Y

from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from mlxtend.plotting import plot_decision_regions
from sklearn.metrics import accuracy_score

def evaluate(clf,X,Y):
  clf.fit(X,Y)
  plot_tree(clf)
  plt.show()
  # plot_decision_regions(X.values,Y.values, clf=clf , legend=2)
  y_pred=clf.predict(X_test)
  y_pred
  #print(accuracy_score(y_test,y_pred))

#@title Default title text
dt_bag1 = DecisionTreeRegressor()
dt_bag11 = DecisionTreeClassifier()
#

"""We're adding out Bag 1 to make a low bias and high variance model"""

# Decision tree Classifier 

evaluate(dt_bag11,X,Y)

#accuracy_score(y_pred,y_test)

evaluate(dt_bag1,X,Y)

#@title Bagging with 3 Decision Trees 
df_bag = dff_train.sample(7,replace=True) 

X= df_bag.iloc[:,0:8]
Y= df_bag.iloc[:,-1]

df_bag

X

#@title Bagging with 3 Decision Trees 2nd Bag
df_bag2 = dff_train.sample(7,replace=True) 

X= df_bag2.iloc[:,0:8]
Y= df_bag2.iloc[:,-1]

df_bag

dt_bag2 = DecisionTreeRegressor()
evaluate(dt_bag2,X,Y)

dt_bag22 = DecisionTreeClassifier()
evaluate(dt_bag22,X,Y)

#@title Bagging with 3 Decision Trees 3rd Bag
df_bag3 = dff_train.sample(7,replace=True) 

X= df_bag3.iloc[:,0:8]
Y= df_bag3.iloc[:,-1]

df_bag3

df_bag3 = DecisionTreeRegressor()
evaluate(df_bag3,X,Y)

from sklearn.ensemble import BaggingRegressor
X_train,X_test,Y_train,Y_test = train_test_split(Cat_1,Cat_2,test_size=0.25,random_state=0)

# now printing the overall sizes of our splitted result
print("Train / Test Sets sizes: ", x_train.shape,x_test.shape,y_train.shape,y_test.shape )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # n_samples = data_read.data.shape[0]
# # n_features = data_read.data.shape[1]
# 
# params = {'base_estimator': [None, df_bag3,df_bag2,df_bag],
#           'n_estimators': [10,15,21],
#           'max_samples': [0.25,0.5],
#           'max_features': [0.5,1.0],
#           'bootstrap': [True, False],
#           'bootstrap_features': [True, False]}
# 
# bagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), param_grid =params, cv=2, n_jobs=-1, verbose=1)
# import warnings
# warnings.filterwarnings('ignore')
# bagging_regressor_grid.fit(X_train, Y_train)
# 
# print('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_train, Y_train))
# 
# print('Overall R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(Cat_1,Cat_2))
# print('Best Parameters : ',bagging_regressor_grid.best_params_)
# 
# 
#

result=bagging_regressor_grid.predict(Cat_1) 
result

Y_test_pred_BAGGING= bagging_regressor_grid.predict(X_test)
Y_train_pred_BAGGING= bagging_regressor_grid.predict(X_train)

#@title Evaluation results  Ensemble Learning Bagging - With Our Hybrid Approach using GridSearchCV  On Multiple Decision Trees

diff=np.absolute(Y_test-Y_test_pred_BAGGING)
MRE=(diff/Y_test)/MRE.size
MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
Pred=(P.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)
Accuracy=(1-MRE)*100
Accuracy=(Accuracy.mean())
# MRE=(diff/y_test)/MRE.size
P2=MRE[MRE<MMRE]
print("PRED MMRE",Pred2)
print("Model Accuracy",Accuracy)

#@title Using Ensemble Technique 1st set of Algorithms

# 1
from sklearn.ensemble import BaggingRegressor

X_train,X_test,Y_train,Y_test = train_test_split(Cat_1,Cat_2,test_size=0.25,random_state=0)

# now printing the overall sizes of our splitted result
print("Train / Test Sets sizes: ", x_train.shape,x_test.shape,y_train.shape,y_test.shape )

lr = LinearRegression()
dt = DecisionTreeRegressor()
knn = KNeighborsRegressor()

lr.fit(X_train,Y_train)
dt.fit(X_train,Y_train)
knn.fit(X_train,Y_train)

y_pred1 = lr.predict(X_test)
y_pred2 = dt.predict(X_test)
y_pred3 = knn.predict(X_test)

print("R^2 score for LR",r2_score(Y_test,y_pred1))
print("R^2 score for DT",r2_score(Y_test,y_pred2))
print("R^2 score for KNN",r2_score(Y_test,y_pred3))

from sklearn.ensemble import BaggingRegressor

bag_regressor = BaggingRegressor(random_state=1)
bag_regressor.fit(X_train, Y_train)

Y_preds = bag_regressor.predict(X_test)

print('Training Coefficient of R^2 : %.3f'%bag_regressor.score(X_train, Y_train))

from sklearn.linear_model import Lasso

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # n_samples = data_read.data.shape[0]
# # n_features = data_read.data.shape[1]
# 
# params = {'base_estimator': [None, LinearRegression(), KNeighborsRegressor()],
#           'n_estimators': [10,15,21],
#           'max_samples': [0.5,0.95],
#           'max_features': [0.5,1.0],
#           'bootstrap': [True, False],
#           'bootstrap_features': [True, False]}
# 
# bagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), param_grid =params, cv=2, n_jobs=-1, verbose=1)
# bagging_regressor_grid.fit(X_train, Y_train)
# 
# print('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_train, Y_train))
# print('Test R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_test, Y_test))
# print('Final R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(Cat_1,Cat_2))
# print('Best Parameters : ',bagging_regressor_grid.best_params_)
# 
# 
#

Y_test_pred_BAGGING= bagging_regressor_grid.predict(X_test)
Y_train_pred_BAGGING= bagging_regressor_grid.predict(X_train)

result=bagging_regressor_grid.predict(Cat_1) 
result

Y_train_pred_BAGGING

#@title Evaluation results  Ensemble Learning Bagging - With Our Hybrid Approach using GridSearchCV 

diff=np.absolute(Y_test-Y_test_pred_BAGGING)
MRE=(diff/Y_test)/MRE.size
MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
Pred=(P.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)
Accuracy=(1-MRE)*100
Accuracy=(Accuracy.mean())
# MRE=(diff/y_test)/MRE.size
P2=MRE[MRE<MMRE]
print("PRED MMRE",Pred2)
print("Model Accuracy",Accuracy)

#@title Third Approach Using Ensemble Learning - Bagging with use of Bootstrapping

#@title Using Ensemble Technique Second set of Algorithms
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.preprocessing import KBinsDiscretizer

X_train,X_test,Y_train,Y_test = train_test_split(Cat_1,Cat_2,test_size=0.30,random_state=0)

# now printing the overall sizes of our splitted result
print("Train / Test Sets sizes: ", x_train.shape,x_test.shape,y_train.shape,y_test.shape )

rkf = RandomForestRegressor()
dt = DecisionTreeRegressor()
ada = AdaBoostRegressor()

from logging import addLevelName
rkf.fit(X_train,Y_train)
dt.fit(X_train,Y_train)
ada.fit(X_train,Y_train)

y_pred1 = rkf.predict(X_test)
y_pred2 = dt.predict(X_test)
y_pred3 = ada.predict(X_test)

print("R^2 score for RF",r2_score(Y_test,y_pred1))
print("R^2 score for DT",r2_score(Y_test,y_pred2))
print("R^2 score for ADA_B",r2_score(Y_test,y_pred3))

from sklearn.ensemble import BaggingRegressor

bag_regressor = BaggingRegressor(random_state=1)
bag_regressor.fit(X_train, Y_train)

Y_preds = bag_regressor.predict(X_test)

print('Coefficient of R^2 : %.3f'%bag_regressor.score(X_train, Y_train))
# print('Test Coefficient of R^2 : %.3f'%bag_regressor.score(X_test, Y_test))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # n_samples = data_read.data.shape[0]
# # n_features = data_read.data.shape[1]
# 
# params = {'base_estimator': [None, LinearRegression(), KNeighborsRegressor(),DecisionTreeRegressor()],
#           'n_estimators': [10,14,21],
#           'max_samples': [0.5,1.0],
#           'max_features': [0.5,1.0],
#           'bootstrap': [True, False],
#           'bootstrap_features': [True, False]}
# 
# bagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), param_grid =params, cv=3, n_jobs=-1, verbose=1)
# bagging_regressor_grid.fit(X_train, Y_train)
# 
# print('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_train, Y_train))
# 
# print('Overall R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(Cat_1,Cat_2))
# print('Best Parameters : ',bagging_regressor_grid.best_params_)
# 
# 
#

Y_test_pred_BAGGING= bagging_regressor_grid.predict(X_test)
Y_train_pred_BAGGING= bagging_regressor_grid.predict(X_train)

result=bagging_regressor_grid.predict(Cat_1) 
result

Y_train_pred_BAGGING

#@title Evaluation results  Ensemble Learning Bagging - With Our Hybrid Approach using GridSearchCV 

diff=np.absolute(Y_test-Y_test_pred_BAGGING)
MRE=(diff/Y_test)/MRE.size
MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
Pred=(P.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)
Accuracy=(1-MRE)*100
Accuracy=(Accuracy.mean())
#MRE=(diff/y_test)/MRE.size
P2=MRE[MRE<MMRE]
print("PRED MMRE",Pred2)
print("Model Accuracy",Accuracy)

#@title Ensemble

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # n_samples = data_read.data.shape[0]
# # n_features = data_read.data.shape[1]
# 
# params = {'base_estimator': [None, LinearRegression(), KNeighborsRegressor()],
#           'n_estimators': [10,15,21],
#           'max_samples': [0.5,1.0],
#           'max_features': [0.5,1.0],
#           'bootstrap': [True, False],
#           'bootstrap_features': [True, False]}
# 
# bagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), param_grid =params, cv=3, n_jobs=-1, verbose=1)
# bagging_regressor_grid.fit(X_train, Y_train)
# 
# print('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_train, Y_train))
# 
# print('Overall R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(Cat_1,Cat_2))
# print('Best Parameters : ',bagging_regressor_grid.best_params_)
# 
# 
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # n_samples = data_read.data.shape[0]
# # n_features = data_read.data.shape[1]
# 
# params = {'base_estimator': [None, LinearRegression(), KNeighborsRegressor()],
#           'n_estimators': [10,15,21],
#           'max_samples': [0.5,1.0],
#           'max_features': [0.5,1.0],
#           'bootstrap': [True, False],
#           'bootstrap_features': [True, False]}
# 
# bagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), param_grid =params, cv=3, n_jobs=-1, verbose=1)
# bagging_regressor_grid.fit(X_train, Y_train)
# 
# print('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_train, Y_train))
# print('Test R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_test, Y_test))
# print('Overall R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(Cat_1,Cat_2))
# print('Best Parameters : ',bagging_regressor_grid.best_params_)
# 
# 
#

Y_test_pred_BAGGING= bagging_regressor_grid.predict(X_test)
Y_train_pred_BAGGING= bagging_regressor_grid.predict(X_train)

result=bagging_regressor_grid.predict(Cat_1) 
result

Y_train_pred_BAGGING

#@title Evaluation results  Ensemble Learning Bagging - With Our Hybrid Approach using GridSearchCV 

diff=np.absolute(Y_test-Y_test_pred_BAGGING)
MRE=(diff/Y_test)/MRE.size
MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
Pred=(P.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)
Accuracy=(1-MRE)*100
Accuracy=(Accuracy.mean())
#MRE=(diff/y_test)/MRE.size
P2=MRE[MRE<MMRE]
print("PRED MMRE",Pred2)
print("Model Accuracy",Accuracy)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # n_samples = data_read.data.shape[0]
# # n_features = data_read.data.shape[1]
# 
# params = {'estimator': [RandomForestRegressor(), DecisionTreeRegressor(), AdaBoostRegressor()],
#           'n_estimators': [10,15,21],
#           'max_samples': [0.5,1.0],
#           'max_features': [0.5,8],
#           'bootstrap': [True, False],
#           'bootstrap_features': [True, False]}
# 
# bagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), param_grid =params, cv=2, n_jobs=-1, verbose=1)
# import warnings
# warnings.filterwarnings('ignore')
# bagging_regressor_grid.fit(X_train, Y_train)
# 
# print('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_train, Y_train))
# #print('Test R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_test, Y_test))
# print('Overall R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(Cat_1,Cat_2))
# print('Best Parameters : ',bagging_regressor_grid.best_params_)
# 
# 
#

Y_test_pred_BAGGING= bagging_regressor_grid.predict(X_test)
Y_train_pred_BAGGING= bagging_regressor_grid.predict(X_train)

result=bagging_regressor_grid.predict(Cat_1) 
result

#@title Evaluation results  Ensemble Learning Bagging - With Our Hybrid Approach using GridSearchCV 

diff=np.absolute(Y_test-Y_test_pred_BAGGING)
MRE=(diff/Y_test)/MRE.size
MMRE=np.mean(MRE)
MdMRE=np.median(MRE)
P=MRE[MRE<.25]
Pred=(P.size/MRE.size) * 100
print("MMRE",MMRE)
print("MdMRE",MdMRE)
print("PRED 25%",Pred)
Accuracy=(1-MRE)*100
Accuracy=(Accuracy.mean())
#MRE=(diff/y_test)/MRE.size
P2=MRE[MRE<MMRE]
print("PRED MMRE",Pred2)
print("Model Accuracy",Accuracy)