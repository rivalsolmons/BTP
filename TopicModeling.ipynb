{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rivalsolmons/BTP/blob/main/TopicModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the pips required:"
      ],
      "metadata": {
        "id": "1fPrDId-Dhcb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4jVX3Ip1pRP"
      },
      "outputs": [],
      "source": [
        "!pip install requests\n",
        "## The below are for LDA and viz of the LDA\n",
        "!pip install pyLDAvis\n",
        "!pip install gensim\n",
        "!pip install pyLDAvis.gensim_models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the imports required for loading videos to the API and retrieving the conversations"
      ],
      "metadata": {
        "id": "B2Rbej0y1re5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V5HwaozYPMEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "from pprint import pprint #pretty-printing of Python data structures, used to print data in a readable format.\n",
        "import requests #for making HTTP requests in Python\n",
        "import glob\n",
        "import os\n",
        "from google.colab import files ## upload a video\n",
        "import time"
      ],
      "metadata": {
        "id": "VkXLLyjN1sFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expressing that we'd like \"conversation\" data and creating an index_id.  We'll be using the conversation for our topic modeling. Creates 2 dictionaries, makes a post request to the INDEXES_URL and stores it in the response variable."
      ],
      "metadata": {
        "id": "S5-LHaDMD9sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_URL = \"https://api.twelvelabs.io/v1.1\"\n",
        "\n",
        "API_KEY = \"[YOU API KEY]\"\n",
        "\n",
        "INDEXES_URL = f\"{API_URL}/indexes\"\n",
        "\n",
        "INDEX_NAME = \"Mr. Beast's Demo\" # Use a descriptive name for your index\n",
        "\n",
        "\n",
        "\n",
        "headers = { \"x-api-key\": API_KEY }\n",
        "\n",
        "data = {\n",
        "    \"engine_id\": \"marengo2\",\n",
        "     \"index_options\":[\"conversation\"],\n",
        "     \"index_name\": INDEX_NAME,\n",
        "}\n",
        "\n",
        "response = requests.post(INDEXES_URL, headers=headers, json=data)\n",
        "INDEX_ID = response.json().get('_id')\n",
        "print (f'Status code: {response.status_code}')\n",
        "pprint (response.json())"
      ],
      "metadata": {
        "id": "oRUFvwTa3EJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploading 4 video files for comparison"
      ],
      "metadata": {
        "id": "kJz9eU5TEE4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Xlxda014mhWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "PJCjKD0rmySm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "HpGBqHOTm3M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4ypkArqgnaQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looping through the different tasks to create a task_id for each one (Mr. Beast, you will need to add the names of your own files here)"
      ],
      "metadata": {
        "id": "yBBGI9ssEJ43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASKS_URL = f\"{API_URL}/tasks\"\n",
        "file_names = [\"TCPDS_Mohamed.mp4\", \"TCPDS_nicola2.mp4\", \"TCPDS_Timothy_datacollection.mp4\", \"yolov5_integration_announcement.mp4\"]\n",
        "tasks_list = []\n",
        "\n",
        "data = {\n",
        "    \"index_id\": INDEX_ID,\n",
        "    \"language\": \"en\"\n",
        "}\n",
        "\n",
        "for file_name in file_names:\n",
        "  file_stream = open(file_name,\"rb\")\n",
        "  file_param=[\n",
        "      (\"video_file\", (file_name, file_stream, \"application/octet-stream\")),]\n",
        "\n",
        "  response = requests.post(TASKS_URL, headers=headers, data=data, files=file_param)\n",
        "  TASK_ID = response.json().get(\"_id\")\n",
        "  tasks_list.append(TASK_ID)\n",
        "  print (f\"Status code: {response.status_code}\")\n",
        "  pprint (response.json())\n"
      ],
      "metadata": {
        "id": "zbXeUFXx5GvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polling to understand when all 4 videos are \"ready\"\n"
      ],
      "metadata": {
        "id": "BOuN4ZVc01rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  all_ready = True\n",
        "  for task in tasks_list:\n",
        "    TASK_STATUS_URL = f\"{API_URL}/tasks/{task}\"\n",
        "    response = requests.get(TASK_STATUS_URL, headers=headers)\n",
        "    STATUS = response.json().get(\"status\")\n",
        "    print(response.json())\n",
        "    if STATUS == \"ready\":\n",
        "      print(\"ready\")\n",
        "    else:\n",
        "      all_ready = False\n",
        "  if all_ready:\n",
        "    break\n",
        "  time.sleep(600)"
      ],
      "metadata": {
        "id": "CVcwSJkG2mhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the conversation results so that we can use the text to find topics in these videos."
      ],
      "metadata": {
        "id": "ekGjXh0c2yFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEARCH_URL = f\"{API_URL}/search\"\n",
        "\n",
        "data = {\n",
        "  \"query\": \"data\",\n",
        "  \"index_id\": INDEX_ID,\n",
        "  \"search_options\": [\"conversation\"],\n",
        "}\n",
        "\n",
        "response = requests.post(SEARCH_URL, headers=headers, json=data)\n",
        "print (f\"Status code: {response.status_code}\")\n",
        "pprint (response.json())"
      ],
      "metadata": {
        "id": "xrXG1Cvd2X8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting analysis of topics.  Placing conversation data in a dataframe then we'll start preprocessing."
      ],
      "metadata": {
        "id": "aFczElbkGfBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import re # Load the regular expression library"
      ],
      "metadata": {
        "id": "xK9lxJvGHFqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = { \"x-api-key\": API_KEY }\n",
        "\n",
        "def list_videos(INDEX_ID):\n",
        "  response = requests.get(f'{API_URL}/indexes/{INDEX_ID}/videos', headers=headers)\n",
        "  return response.json()\n",
        "\n",
        "def retrieve_transcription(INDEX_ID, video_id):\n",
        "  response = requests.get(f'{API_URL}/indexes/{INDEX_ID}/videos/{video_id}/transcription', headers=headers)\n",
        "  return response.json()\n",
        "\n",
        "\n",
        "videos_response = list_videos(INDEX_ID)\n",
        "videos = videos_response['data']\n",
        "rows = []\n",
        "for video in videos:\n",
        "  name = video['metadata']['filename']\n",
        "  transcription_response = retrieve_transcription(INDEX_ID, video['_id'])\n",
        "  for blurb in transcription_response['data']:\n",
        "    rows.append([name, blurb['value']])\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df = df.rename(columns={0: \"video\", 1: \"text\"})\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "asYGXmyIW_cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_processed'] = \\\n",
        "df['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "# Convert the titles to lowercase\n",
        "df['text_processed'].map(lambda x: x.lower())\n",
        "# Print out the first rows of papers\n",
        "df['text_processed'].head()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "eG-OsAKhjnRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the libraries required for the modeling piece of this analysis"
      ],
      "metadata": {
        "id": "fRkKLgZ8FPBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim #NLP library for topic modeling (LDA)\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models import hdpmodel, ldamodel\n",
        "from gensim.utils import simple_preprocess #basic text preprocessing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords #we'll use for removing stopwords\n",
        "import gensim.corpora as corpora\n",
        "import pyLDAvis #Gensim tool for LDA model visualization\n",
        "import pyLDAvis.gensim_models as lda\n",
        "from pprint import pprint\n",
        "import pickle\n",
        "import random # to set a random seed\n"
      ],
      "metadata": {
        "id": "GzandP-cqKo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the words that aren't helpful in the analysis of topics"
      ],
      "metadata": {
        "id": "2iuvfw17HdJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['hey','um','would','one','really','still','every','get','also','says','like','need','make','go','uh','yeah','using','use', 'want','usually','see','something','helped','used',\n",
        "                   'find','think','nothing','related','comment', 'give','know','first','different','nice','new','hub','actually','bunch','going','say','quite','anytime','high','field','works',\n",
        "                   'things','always','set','bit','trying','got','lot','okay','way','ever','reason','able','wondering'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "data = df.text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "print(data_words)\n"
      ],
      "metadata": {
        "id": "DsQ68TA4qRt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "# View\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "SjMKtzw1q7NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of topics\n",
        "num_topics = 6\n",
        "\n",
        "# Build LDA model\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "## Print the topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "id": "5ayL71w9rHfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "LDAvis_data_filepath = os.path.join('.content'+str(num_topics))\n",
        "\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = lda.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "pyLDAvis.save_html(LDAvis_prepared, './content'+ str(num_topics) +'.html')\n",
        "LDAvis_prepared"
      ],
      "metadata": {
        "id": "QpRZVUjXrb9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}